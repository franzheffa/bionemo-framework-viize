{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![ Click here to deploy.](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/launchable/deploy?launchableID=env-2p8Dz9VJ2kFSrTQMXVRtUMgymKx)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM-2 Inference\n",
    "\n",
    "This tutorial serves as a demo for [ESM2](https://www.science.org/doi/abs/10.1126/science.ade2574) Inference using a CSV file with `sequences` column. To pre-train the ESM2 model please refer to [ESM-2 Pretraining](./pretrain.md) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this tutorial, we will demonstrate how to download ESM2 checkpoint, create a CSV file with protein sequences, and infer a ESM-2 model.\n",
    "\n",
    "All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. For more information on how to build or pull the BioNeMo2 container, refer to the [Initialization Guide](../../getting-started/initialization-guide.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the work directory to store data and results:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> We set the following to clean up the work directory created by this notebook  <pre>cleanup : bool = True</pre></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/workspace/bionemo2/esm2_inference_tutorial' created.\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"/workspace/bionemo2/esm2_inference_tutorial\"\n",
    "\n",
    "if cleanup and os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir)\n",
    "\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "    print(f\"Directory '{work_dir}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{work_dir}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download the pre-trained model, `esm2n/650m:2.0`, from the NGC registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/bionemo/0798767e843e3d54315aef91934d28ae7d8e93c2849d5fcfbdf5fac242013997-esm2_650M_nemo2.tar.gz.untar\n"
     ]
    }
   ],
   "source": [
    "from bionemo.core.data.load import load\n",
    "\n",
    "\n",
    "checkpoint_path = load(\"esm2/650m:2.0\")\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use the `InMemoryProteinDataset` class to load the protein sequence data from a `.csv` file. This data file should at least have a `sequences` column and can optionally have a `labels` column used for fine-tuning applications. Here is an example of how to create your own inference input data using a list of sequences in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_sequence_data = [\n",
    "    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n",
    "    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"sequences.csv\")\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PyTorch Lightning, ESM-2 Inference takes advantage of some key classes:\n",
    "\n",
    "1. `MegatronStrategy` - To launch and setup parallelism for [NeMo](https://github.com/NVIDIA/NeMo/tree/main) and [Megatron-LM](https://github.com/NVIDIA/Megatron-LM).\n",
    "2. `Trainer` - To configure training configurations and logging.\n",
    "3. `ESMFineTuneDataModule` - To load sequence data for both fine-tuning and inference.\n",
    "4. `ESM2Config` - To configure the ESM-2 model as `BionemoLightningModule`.\n",
    "\n",
    "Please refer to [ESM-2 Pretraining](./pretrain.md) and [ESM-2 Fine-Tuning](./finetune.md) tutorials for detailed description of these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run inference on the data created in the previous step, we can use the `infer_esm2` executable which calls `bionemo-framework/sub-packages/bionemo-esm2/src/bionemo/esm2/scripts/infer_esm2.py`. We can get a full description of inference arguments by providing `--help` in the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: infer_esm2 [-h] --checkpoint-path CHECKPOINT_PATH --data-path DATA_PATH\n",
      "                  --results-path RESULTS_PATH\n",
      "                  [--precision {fp16,bf16,fp32,bf16-mixed,fp32-mixed,16-mixed,fp16-mixed,16,32}]\n",
      "                  [--num-gpus NUM_GPUS] [--num-nodes NUM_NODES]\n",
      "                  [--micro-batch-size MICRO_BATCH_SIZE]\n",
      "                  [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]\n",
      "                  [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]\n",
      "                  [--prediction-interval {epoch,batch}] [--include-hiddens]\n",
      "                  [--include-input-ids] [--include-embeddings]\n",
      "                  [--include-logits] [--config-class CONFIG_CLASS]\n",
      "                  [--lora-checkpoint-path LORA_CHECKPOINT_PATH]\n",
      "\n",
      "Infer ESM2.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --checkpoint-path CHECKPOINT_PATH\n",
      "                        Path to the ESM2 pretrained checkpoint\n",
      "  --data-path DATA_PATH\n",
      "                        Path to the CSV file containing sequences and label\n",
      "                        columns\n",
      "  --results-path RESULTS_PATH\n",
      "                        Path to the results directory.\n",
      "  --precision {fp16,bf16,fp32,bf16-mixed,fp32-mixed,16-mixed,fp16-mixed,16,32}\n",
      "                        Precision type to use for training.\n",
      "  --num-gpus NUM_GPUS   Number of GPUs to use for training. Default is 1.\n",
      "  --num-nodes NUM_NODES\n",
      "                        Number of nodes to use for training. Default is 1.\n",
      "  --micro-batch-size MICRO_BATCH_SIZE\n",
      "                        Micro-batch size. Global batch size is inferred from\n",
      "                        this.\n",
      "  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n",
      "                        Pipeline model parallel size. Default is 1.\n",
      "  --tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE\n",
      "                        Tensor model parallel size. Default is 1.\n",
      "  --prediction-interval {epoch,batch}\n",
      "                        Intervals to write DDP predictions into disk\n",
      "  --include-hiddens     Include hiddens in output of inference\n",
      "  --include-input-ids   Include input_ids in output of inference\n",
      "  --include-embeddings  Include embeddings in output of inference\n",
      "  --include-logits      Include per-token logits in output.\n",
      "  --config-class CONFIG_CLASS\n",
      "                        Model configs link model classes with losses, and\n",
      "                        handle model initialization (including from a prior\n",
      "                        checkpoint). This is how you can fine-tune a model.\n",
      "                        First train with one config class that points to one\n",
      "                        model class and loss, then implement and provide an\n",
      "                        alternative config class that points to a variant of\n",
      "                        that model and alternative loss. In the future this\n",
      "                        script should also provide similar support for picking\n",
      "                        different data modules for fine-tuning with different\n",
      "                        data types. Choices: dict_keys(['ESM2Config',\n",
      "                        'ESM2FineTuneSeqConfig', 'ESM2FineTuneTokenConfig'])\n",
      "  --lora-checkpoint-path LORA_CHECKPOINT_PATH\n",
      "                        Path to the lora states to restore from.\n"
     ]
    }
   ],
   "source": [
    "! infer_esm2 --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The hidden states (which are usually the output of each layer in a neural network) can be obtained by using `--include-hiddens` argument when calling the inference function of ESM-2 in BioNeMo Framework.\n",
    "\n",
    "The hidden states can be converted into fixed-size vector embeddings. This is done by removing the hidden state vectors corresponding to padding tokens, then averaging across the rest. This process is often used when the goal is to create a single vector representation from the hidden states of a model, which can be used for various sequence-level downstream tasks such as classification (e.g. subcellular localization) or regression (e.g. melting temperature prediction). To obtain the embedding results we can use `--include-embeddings` argument.\n",
    "\n",
    "By passing the hidden state of an amino acid sequence through the BERT language model head, we can obtain output logits at each position and transform them into probabilities. This can happen by using `--include-logits` argument. Logits here are the raw, unnormalized scores that represent the likelihood of each class and are not probabilities themselves; they can be any real number, including negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets call `infer_esm2` executable with relevant arguments to compute and optionally return embeddings, hiddens and logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "! infer_esm2 --checkpoint-path {checkpoint_path} \\\n",
    "             --data-path {data_path} \\\n",
    "             --results-path {work_dir} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --precision \"bf16-mixed\" \\\n",
    "             --include-hiddens \\\n",
    "             --include-embeddings \\\n",
    "             --include-logits \\\n",
    "             --include-input-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference predictions are stored into `.pt` files for each device. Since we only used one device to run the inference (`--num-gpus 1`) in the previous step, the results were written to `{work_dir}/predictions__rank_0.pt` under the work directory of this notebook (defined above). The `.pt` file containes a dictionary of `{'result_key': torch.Tensor}` that be loaded with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_logits\ttorch.Size([1024, 10, 128])\n",
      "hidden_states\ttorch.Size([10, 1024, 1280])\n",
      "input_ids\ttorch.Size([10, 1024])\n",
      "embeddings\ttorch.Size([10, 1280])\n"
     ]
    }
   ],
   "source": [
    "results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n",
    "\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f\"{key}\\t{val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example `data` a python dict with the following keys `['token_logits', 'hidden_states', 'input_ids', 'embeddings']`. Logits (`token_logits`) tensor has a dimension of `[sequence, batch, hidden]` to improve the training performance. We will transpose the first two dimension in the following to have batch-first shape like the rest of the output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "logits = results[\"token_logits\"].transpose(0, 1)  # s, b, h  -> b, s, h\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension of `token_logits` is 128, with the first 33 positions corresponding to the amino acid vocabulary, followed by 95 paddings. We use the `tokenizer.vocab_size` to filter out the paddings and only keep the 33 vocab positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 unique tokens: ['<cls>', '<pad>', '<eos>', '<unk>', 'L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-', '<null_1>', '<mask>'].\n",
      "Logits shape after removing the paddings in hidden dimension: torch.Size([10, 1024, 33])\n"
     ]
    }
   ],
   "source": [
    "from bionemo.esm2.data.tokenizer import get_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "tokens = tokenizer.all_tokens\n",
    "print(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")\n",
    "\n",
    "aa_logits = logits[..., : tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\n",
    "print(f\"Logits shape after removing the paddings in hidden dimension: {aa_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set aside the tokens corresponding to the 20 known amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_tokens = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\n",
    "\n",
    "aa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\n",
    "extra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence dimension in this example (1024) is representing the max sequence length wich includes paddings, EOS, and BOS. To filter the relevant amino acid information we can use the input sequence IDs in the results to create a mask that can be used to extract the relevant information in `aa_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = results[\"input_ids\"]  # b, s\n",
    "# mask where non-amino acid tokens are True\n",
    "mask = torch.isin(input_ids, torch.tensor(extra_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with LoRA\n",
    "Inference with LoRA is supported. This requires the original model weights along with the additional LoRA weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:42 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.PREDICTING\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Found model_transform attribute on pl_module\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x70af0b61c220>\n",
      "[WARNING  | /workspaces/bionemo-framework/sub-packages/bionemo-llm/src/bionemo/llm/model/config.py]: Loading /home/ubuntu/.cache/bionemo/2957b2c36d5978d0f595d6f1b72104b312621cf0329209086537b613c1c96d16-esm2_hf_converted_8m_checkpoint.tar.gz.untar\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Padded vocab_size: 128, original vocab_size: 33, dummy tokens: 95.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-04-25 04:47:43 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 7542848\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.0.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.0.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.0.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.0.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.1.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.1.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.1.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.1.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.2.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.2.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.2.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.2.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.3.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.3.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.3.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.3.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.4.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.4.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.4.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.4.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.5.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.5.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.5.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Adding lora to: 0.module.module.module.encoder.layers.5.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] After applying model_transform:\n",
      "    \n",
      "  | Name   | Type | Params | Mode\n",
      "---------------------------------------\n",
      "0 | module | DDP  | 8.5 M  | eval\n",
      "---------------------------------------\n",
      "1.1 M     Trainable params\n",
      "7.4 M     Non-trainable params\n",
      "8.5 M     Total params\n",
      "34.104    Total estimated model params size (MB)\n",
      "121       Modules in train mode\n",
      "133       Modules in eval mode\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Loading adapters from /home/ubuntu/.cache/bionemo/ffce714478009c353f20e8c1ad8e49638128f0cc936ebe1d44c161ac89831dcb-finetuned_peft_weights.tar.gz.untar/weights\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Initializing model parallel\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 8525888\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393]  > number of trainable parameters: 1086528 (12.74% of total)\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x70af081e3f50> dist-ckpt load strategy.\n",
      "[NeMo I 2025-04-25 04:47:43 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1745556463.638s : Time spent in load_checkpoint: 0.029s\n",
      "[NeMo W 2025-04-25 04:47:43 nemo_logging:405] MegatronOptimizerModule not found in trainer callbacks. finalize_model_grads is not properly set up for PEFT.\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and PEFT weights\n",
    "\n",
    "checkpoint_path = load(\"esm2/8m:2.0\")\n",
    "lora_checkpoint_path = load(\"esm2/esm2_lora_weights:1.1\") / \"weights\"\n",
    "\n",
    "# Perform inference with LoRA\n",
    "! infer_esm2 --checkpoint-path {checkpoint_path} \\\n",
    "             --data-path {data_path} \\\n",
    "             --results-path {work_dir} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --include-hiddens \\\n",
    "             --include-embeddings \\\n",
    "             --include-logits \\\n",
    "             --include-input-ids \\\n",
    "             --lora-checkpoint-path {lora_checkpoint_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_logits\ttorch.Size([1024, 10, 128])\n",
      "hidden_states\ttorch.Size([10, 1024, 320])\n",
      "input_ids\ttorch.Size([10, 1024])\n",
      "embeddings\ttorch.Size([10, 320])\n"
     ]
    }
   ],
   "source": [
    "results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n",
    "\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f\"{key}\\t{val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDP Inference Support\n",
    "\n",
    "Although this tutorial is utilizing one devive to run the inference, distributed inference is supported for ESM2 in BioNeMo Framework. One can simply set the the `--num-gpus n` to run distributed inference on `n` devices. The output predictions will be written into `predictions__rank_<0...n-1>.pt` under the `--results-path` provided. Moreover, by optionally including input token IDs with `--include-input-ids` we can snure 1:1 mapping between input sequences and output predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet can be used to load and collate the predictions into a single dictionary.\n",
    "\n",
    "\n",
    "```python\n",
    "import glob\n",
    "from bionemo.llm.lightning import batch_collator\n",
    "\n",
    "collated_preditions = batch_collator([torch.load(path) for path in glob.glob(f\"{work_dir}/predictions__rank_*.pt\")])\n",
    "for key, val in collated_preditions.items():\n",
    "    if val is not None:\n",
    "        print(f'{key}\\t{val.shape}')\n",
    "\n",
    "# token_logits\ttorch.Size([1024, 10, 128])\n",
    "# hidden_states\ttorch.Size([10, 1024, 1280])\n",
    "# input_ids     torch.Size([10, 1024])\n",
    "# embeddings\ttorch.Size([10, 1280])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more in-depth example of inference and converting logits to probabilities please refer to [ESM-2 Mutant Design Tutorial](./mutant-design.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
